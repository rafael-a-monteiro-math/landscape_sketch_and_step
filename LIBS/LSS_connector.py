#!/usr/bin/env python

#####################################################################
# This code is part of the simulations done for the paper
# 'Landscape-Sketch-Step: An AI/ML-Based Metaheuristic
#     for Surrogate Optimization Problems',
# by Rafael Monteiro and Kartik Sau
#
# author : Rafael Monteiro
# affiliation : Mathematics for Advanced Materials -
#               Open Innovation Lab(MathAM-OIL, AIST)
#               Sendai, Japan
# email : rafael.a.monteiro.math@gmail.com
# date : July 2023
#
#####################################################################

__author__="Rafael de Araujo Monteiro"
__affiliation__=\
    """Mathematics for Advanced Materials - Open Innovation Lab,
        (Matham-OIL, AIST),
        Sendai, Japan"""
__copyright__="None"
__credits__=["Rafael Monteiro"]
__license__=""
__version__="0.0.0"
__maintainer__="Rafael Monteiro"
__email__="rafael.a.monteiro.math@gmail.com"
__github__="https://github.com/rafael-a-monteiro-math/"
__date__=""
#####################################################################
# IMPORT LIBRARIES
#####################################################################

import subprocess
import numpy as np
import pandas as pd
#from LIBS.Deep_learning_MD_lib import Extract
import tensorflow as tf
from loguru import logger

from .LSS_print import append_to_log_file, save_as_npy, load_npy, print_parameters

################################################################################
# REMARK
###
# The following script runs as follows
###
# "./script_prl_2022.sh" arg1 arg2 arg3
###
# where
# arg1 indicates the parameter file read, from file parameter_$1.txt
###
# arg2 indicates the configuration used, read from read$2.dat
###
# arg3 indicates the extension of the output file, printed as force_$3.dat
###
#########################################
# EVALUATION AND METRICS
#########################################


def l2_norm(matrix_1, matrix_2):
    """
    In LSS_connector.py.
    
    Inputs are all numpy matrices.
    Due to loss of significan digits, the computations will be as 
    pointed out in the worklog remark
    """
    l_2_norms_per_config=tf.linalg.norm(matrix_1 - matrix_2, axis=[1,2])
    n_atoms=matrix_1.shape[0]

    return (1 / n_atoms) * l_2_norms_per_config


def big_evaluation(X, configurations, energy, data=[]):
    """
    Args:
        X (_type_): _description_
        configurations (_type_): _description_
        E (_type_): _description_
        data (list, optional): _description_. Defaults to[].

    Returns:
        _type_: _description_
    if issubclass(energy, "Extract"):
        # Do something if some_class is a subclass of BaseClass
        print_parameters(X, in_parts=True)
    else:
        # Do something else if some_class is not a subclass of BaseClass
        raise ValueError(
            "The third argument must be from the 'Extract' class.")

    """
    # At first we print the parameters as several txt files
    
    number_parameters=X.shape[0]
    y=0
    mean_on_test=[]
    if not data:
        # NO VALIDATION
        #
        # In this case, parameter_i.txt corresponds to force_i.dat,
        # which will be generated by call_script
        call_script(configurations, parameter_number=-1)
        # Need to retrieve force and calculate, which is done
        # with the next function
        y, _=little_evaluation(configurations, energy, data=data)
        number_evaluations=number_parameters
    else:
        # VALIDATION
        # In this case, for each parameter
        # At each one of them we will evaluate several different configurations
        for i in range(number_parameters):
            # generate forces
            # In this case, parameter_i.txt will be assocaited with
            # several force files
            # force _[0... len(configurations)-1].dat,
            # the output of the next function
            call_script(configurations, parameter_number=i)
            _, mean_on_test_now=little_evaluation(
                configurations, energy, data=data)
            # Need to print to the log file.
            mean_on_test.append(mean_on_test_now)

        number_evaluations=number_parameters * len(configurations)

    return y, mean_on_test, number_evaluations


def little_evaluation(configurations, energy, data=None):
    """
    In LSS_connector.py.
    
    Gather parameters in file 'parameter_name' and evaluate the 
    configurations in test for the ab_initio matrix

    data is assumed to be in tensorflow format

    E=Extract, already contains ab_initio and ab_initio_test
    """
    # STEPS
    # 1) Print parameters to file
    if not data:
        # Retrieve Ab_initio at positions test.
        ab_initio_matrix_now=energy.at_configurations(configurations)
    else:
        # Retrieve Ab_initio at positions test.
        ab_initio_matrix_now=data
    # 2) Retrieve Force at positions test.
    force=energy.force_matrix_linalg(len(configurations))
    # 3) Compare norm
    compare_ab_empir=l2_norm(ab_initio_matrix_now, force)
    # 4) Return evaluation
    return compare_ab_empir, tf.reduce_mean(compare_ab_empir)


def read_parameter_files(prefix="Box"):
    """
    In LSS_connector.py.
    
    Read BOX parameters from txt file
    """
    ext="_parameters.txt"
    lower=tf.reshape(tf.constant(
        np.loadtxt(prefix + "_lower" + ext, dtype=np.float32)),(1, -1))
    upper=tf.reshape(tf.constant(
        np.loadtxt(prefix + "_upper" + ext, dtype=np.float32)),(1, -1))
    
    return tf.concat((lower, upper), axis=0)


def run_and_retrieve(
    LSS, input_name, output_name, program_name="query.py", x_active_agents=None):
    """
    In LSS_connector.py.
    
    Print active_agents_x data as npy file.
    Rund query and return its output.
    """
    if not tf.is_tensor(x_active_agents) and x_active_agents.shape[0]==0:
        # In this case, read from a file and
        # save vector as x.
        x_active_agents=LSS.active_agents_x

    how_many=x_active_agents.shape[0]

    if LSS.with_configurations:
        # USED FOR MD SIMULATIONS!
        #
        # Generate new configurations
        # This is just necessary to evaluate in case LSS is not an object
        # form the LandscapeSketchandStep class

        new_config=[LSS.new_configuration() for _ in range(how_many)]
        
        print_new_config=" "
        for cf in new_config:
            print_new_config +=str(cf) + " "
        
        append_to_log_file(
            LSS.log_output,
            "\nMD evaluating at configurations: " + print_new_config,
            verbose=LSS.verbose)
        y, _, number_evaluations=\
            big_evaluation(x_active_agents, new_config, LSS.extractor, data=[])
        
        LSS.evaluation_counter +=number_evaluations
    else:
        save_as_npy(x_active_agents, input_name)
        # Need to print to configurations file
        subprocess.run(
           ['python3', program_name, input_name, output_name],
            check=False)
        # retrieve data
        y=load_npy(output_name)
        LSS.evaluation_counter +=how_many

    # Count evaluations here!!

    return tf.reshape(y,(-1, 1))

######################################################################
# BOX INITIALIZATION AND BOX PRUNNING


def sample_from_box(box, n_sample_points):
    """
    In LSS_connector.py.
    
    """
    system_dimension=box.shape[1]
    sample_x=tf.Variable(np.random.uniform(
        low=box[0], high=box[1],
        size=(n_sample_points, system_dimension)), dtype=tf.float32)
    return sample_x

######################################################################

def call_script(configurations, parameter_number=0):
    """
    In LSS_connector.py.
    
    Call MD script that generates forces at configurations.
    Generates a force_i.dat file for configuration[i].

    When parameter_number==0 we are testing a single parameter, 
    printed as 'parameters_0.txt', against many different configurations.

    When parameters_number !=0, we are running an MD simulation of
    parameters_k.txt 
    on atomic configurations configurations[k], 
    generating an output 'force_k.dat'
    """
    if parameter_number < 0:
        #This is the case where parameters_k has an associated configuration
        #read from the file read_k.dat.
        #In this case, the output will be force_k.dat.
        for output_ext, config in enumerate(configurations):
            _=subprocess.call(
                [".././script_prl_2022.sh",
                str(output_ext), str(config), str(output_ext)],
                check=False)
    else:
        #This is the case where parameters_k will be associated with several outputs
        for output_ext, config in enumerate(configurations):
            _=subprocess.call(
               [".././script_prl_2022.sh",
                str(parameter_number), str(config), str(output_ext)],
                check=False)
    
    logger.info("All forces have been generated!")


def evaluate_at_validation_data(
    LSS, x_data=None, y_data=None, box_search_epoch=0):
    """
    In LSS_connector.py.
    """
    # if not issubclass(LSS, "LandscapeSketchandStep"):
    #     # Do something if some_class is a subclass of BaseClass
    #     # Do something else if some_class is not a subclass of BaseClass
    #     raise ValueError(
    #         "The first argument must belong to the 'LandscapeSketchandStep' class.")

    append_to_log_file(
        LSS.log_output, "\nEvaluating at the validation data",
        verbose=LSS.verbose)
    mean_on_test=[]
    number_evaluations=0
    if tf.reduce_all(y_data is None):
        if tf.reduce_all(x_data is None):
            # Standard case. No validation data, just store nothing
            append_to_log_file(
                LSS.log_output, "\nValidation data is empty!\n",
                verbose=LSS.verbose)
        else:
            # In this case, X corresponds to configurations
            # Retrieve validation data configurations
            ab_initio_matrix_val_data=\
                LSS.extractor.at_configurations(x_data)

            append_to_log_file(
                LSS.log_output,
                "\nEvaluating parameters on the validation set.",
                verbose=LSS.verbose)

            _, mean_on_test, number_evaluations=\
                big_evaluation(
                    LSS.best_x, x_data, LSS.extractor,
                    data=ab_initio_matrix_val_data)

            # Need to print to the log file.
            print_message=\
                "\nSaving the mean on the configurations test set."
            append_to_log_file(
                LSS.log_output,
                print_message, verbose=LSS.verbose)
    else:
        logger("Put your evaluation function here")
        # In this part you should put your connector to your model.
        # It should 'score' your model at the output parameter(s) obtained by
        # the Landscape sketch method.

    # New we need to append to the history
    LSS.history[str(box_search_epoch)].update({
        "mean_on_validation" : mean_on_test
    })
    LSS.evaluation_counter +=number_evaluations

    return mean_on_test
